{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "m2_lab.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMHCVq22Me0c"
      },
      "source": [
        "## M2 Lab\n",
        "\n",
        "Below are a series of tasks to complete with PyTorch. In all problems, unless explicitly stated, you should use vectorized operations. This means you should not have any loops, list comprehensions, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1utT5QAQMe0l"
      },
      "source": [
        "**Complete this section to reflect your group's attendance**\n",
        "\n",
        "Group Members present the entire time:\n",
        "- Person A\n",
        "- Person B\n",
        "\n",
        "Group Members absent the entire time:\n",
        "- Person C\n",
        "\n",
        "Group Members present for part of the time (add brief explanation of which parts):\n",
        "- Person D (not present for questions 5-14)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J95PQ9jzMe0m"
      },
      "source": [
        "#### Some sample tensors referenced later:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bThU9apBMe0l"
      },
      "source": [
        "import torch\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrTyvYcjMe0n"
      },
      "source": [
        "a = torch.tensor(range(0,10))\n",
        "\n",
        "b = torch.tensor([[11,12,13,14],\n",
        "                  [21,22,23,24],\n",
        "                  [31,32,33,34]], dtype=torch.int32)\n",
        "\n",
        "c = torch.tensor([[[111, 112, 113, 114],\n",
        "                   [121, 122, 123, 124]],\n",
        "\n",
        "                  [[211, 212, 213, 214],\n",
        "                   [221, 222, 223, 224]],\n",
        "\n",
        "                  [[311, 312, 313, 314],\n",
        "                   [321, 322, 323, 324]]], dtype=torch.int32)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nfuUARoSoXy"
      },
      "source": [
        "### Part I: M1 Lab Revisited\n",
        "The first 25 question are the same as those in the M1 Lab, except this time you must use torch operations acting upon torch tensors instead of numpy operations acting upon numpy ndarrays."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNThxF_EMe0n"
      },
      "source": [
        "##### 1) Create a (2,2,2) tensor of all zeros, with type float32\n",
        "\n",
        "(A solution is provided to this problem as an illustrative example.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfc1HU0UMe0o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64176791-dcc9-4e37-8e31-306eba6c4191"
      },
      "source": [
        "torch.zeros((2,2,2),dtype=torch.float32)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0., 0.],\n",
              "         [0., 0.]],\n",
              "\n",
              "        [[0., 0.],\n",
              "         [0., 0.]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtreC9c2Me0p"
      },
      "source": [
        "##### 2) Create a (8,16) tensor where each element equals $\\pi$, with type float32\n",
        "\n",
        "(A solution is provided to this problem as an illustrative example.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "zOxbvQu4Me0q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb62c846-324a-4393-f004-0680570ecd59"
      },
      "source": [
        "torch.full((8,16),torch.tensor(np.pi),dtype=torch.float32)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[3.1416, 3.1416, 3.1416, 3.1416, 3.1416, 3.1416, 3.1416, 3.1416, 3.1416,\n",
              "         3.1416, 3.1416, 3.1416, 3.1416, 3.1416, 3.1416, 3.1416],\n",
              "        [3.1416, 3.1416, 3.1416, 3.1416, 3.1416, 3.1416, 3.1416, 3.1416, 3.1416,\n",
              "         3.1416, 3.1416, 3.1416, 3.1416, 3.1416, 3.1416, 3.1416],\n",
              "        [3.1416, 3.1416, 3.1416, 3.1416, 3.1416, 3.1416, 3.1416, 3.1416, 3.1416,\n",
              "         3.1416, 3.1416, 3.1416, 3.1416, 3.1416, 3.1416, 3.1416],\n",
              "        [3.1416, 3.1416, 3.1416, 3.1416, 3.1416, 3.1416, 3.1416, 3.1416, 3.1416,\n",
              "         3.1416, 3.1416, 3.1416, 3.1416, 3.1416, 3.1416, 3.1416],\n",
              "        [3.1416, 3.1416, 3.1416, 3.1416, 3.1416, 3.1416, 3.1416, 3.1416, 3.1416,\n",
              "         3.1416, 3.1416, 3.1416, 3.1416, 3.1416, 3.1416, 3.1416],\n",
              "        [3.1416, 3.1416, 3.1416, 3.1416, 3.1416, 3.1416, 3.1416, 3.1416, 3.1416,\n",
              "         3.1416, 3.1416, 3.1416, 3.1416, 3.1416, 3.1416, 3.1416],\n",
              "        [3.1416, 3.1416, 3.1416, 3.1416, 3.1416, 3.1416, 3.1416, 3.1416, 3.1416,\n",
              "         3.1416, 3.1416, 3.1416, 3.1416, 3.1416, 3.1416, 3.1416],\n",
              "        [3.1416, 3.1416, 3.1416, 3.1416, 3.1416, 3.1416, 3.1416, 3.1416, 3.1416,\n",
              "         3.1416, 3.1416, 3.1416, 3.1416, 3.1416, 3.1416, 3.1416]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTjhzwUyMe0r"
      },
      "source": [
        "##### 3) Check the dimensions of $a$ (okay to get a torch.Size object)\n",
        "\n",
        "(A solution is provided to this problem as an illustrative example.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6STq2Q9HMe0s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5129e68-3158-4b59-8fbc-381220da1194"
      },
      "source": [
        "a.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bZqzfRkMe0s"
      },
      "source": [
        "#### 4) Test whether $a = a^T$ (superscript $T$ denotes transpose).\n",
        "\n",
        "(A solution is provided to this problem as an illustrative example.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsOqWj_iMe0t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd67fd5e-a149-48eb-c8d3-b579ba353ad2"
      },
      "source": [
        "torch.equal(a,a.T)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjXE-fU3Me0t"
      },
      "source": [
        "#### 5) Reshape $a$ into $(1,10)$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1e5CesyMe0u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f475ae3-8851-4d79-fb01-65f2d2cb6098"
      },
      "source": [
        "torch.reshape(a, (1,10))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxKScAszMe0v"
      },
      "source": [
        "#### 6) Test whether $a$ reshaped into $(1,10)$ equals $a$ reshaped into $(10,1)$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-8qMN6mMe0v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84a6fd89-f1a0-4204-d238-e4a5ce0231dd"
      },
      "source": [
        "torch.equal(torch.reshape(a, (1,10)), torch.reshape(a, (1,10)))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZxaL-tZMe0v"
      },
      "source": [
        "#### 7) Reshape c into a one-dimensional tensor, then sort it, then reshape it back to its original dimensions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BM240xEPMe0w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d71b503-2871-47eb-eb77-93227422474d"
      },
      "source": [
        "#torch.reshape(c.reshape((-1)), (3,2,4))\r\n",
        "sorted, indices = torch.sort(c.reshape((-1))) \r\n",
        "torch.reshape(sorted,(3,2,4) )\r\n",
        "#torch.reshape(torch.sort(c.reshape((-1))),(3,2,4))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[111, 112, 113, 114],\n",
              "         [121, 122, 123, 124]],\n",
              "\n",
              "        [[211, 212, 213, 214],\n",
              "         [221, 222, 223, 224]],\n",
              "\n",
              "        [[311, 312, 313, 314],\n",
              "         [321, 322, 323, 324]]], dtype=torch.int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cc97RrHMe0w"
      },
      "source": [
        "#### 8) Compute the dot product of $a$ with itself (i.e., $\\langle a,a \\rangle$)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJ00bgbOMe0w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95fadea2-7034-450b-a6be-97592f2e3e28"
      },
      "source": [
        "torch.dot(a,a)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(285)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FZw3L9BMe0w"
      },
      "source": [
        "#### 9) Compute the Frobenius inner product of $b$ with itself (i.e., $\\langle b,b \\rangle$)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-C3z7xrMe0x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31d85d09-1166-40a9-be36-6ff7ccdd2fa3"
      },
      "source": [
        "#element wise = torch.mul(A,A)\r\n",
        "torch.mul(b,b).sum()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(6890)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WErDZ21Me0x"
      },
      "source": [
        "#### 10) Matrix multiply $b$ and $b^T$. Note that this is not element-wise multiplication; if you're hazy on matrix multiplication, check out https://en.wikipedia.org/wiki/Matrix_multiplication."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hteFdINZMe0x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c43db07-06ae-4dff-cfd1-0871f3b64bb6"
      },
      "source": [
        "torch.mm(b, torch.transpose(b, 0, 1))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 630, 1130, 1630],\n",
              "        [1130, 2030, 2930],\n",
              "        [1630, 2930, 4230]], dtype=torch.int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ha4ydG1EMe0x"
      },
      "source": [
        "#### 11) Test whether the shape of the matrix multiplication of $b$ and $b^T$ is the same as the shape of the matrix multiplication of $b^T$ and $b$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TC0jsrG7Me0y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8963168c-05f9-4cc7-dbd6-fa384310570f"
      },
      "source": [
        "torch.mm(b, torch.transpose(b, 0, 1)).shape == torch.mm(torch.transpose(b, 0, 1), b).shape"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPeqcCipMe0y"
      },
      "source": [
        "#### 12) Generate a (10000,1) tensor, where each element is random, drawn from a standard normal distribution with mean $\\mu=1.5$ and standard deviation $\\sigma=0.5$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6QMSZ8jMe0y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "782f7386-78a6-4b33-b425-92dfe1094d14"
      },
      "source": [
        "torch.empty(10000).normal_(mean = 1.5, std = 0.5)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.5710, 1.7515, 1.5111,  ..., 0.9610, 2.0702, 1.6604])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80Jy7cOhMe0y"
      },
      "source": [
        "#### 13) Generate the same tensor as in the previous problem, and then compute its standard deviation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlzTmd-JMe0z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fa4cd89-3c56-4ab1-eede-7de4af321839"
      },
      "source": [
        "torch.empty(10000).normal_(mean = 1.5, std = 0.5).std()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.5052)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3OpfPb0Me0z"
      },
      "source": [
        "#### 14) Index into b to extract the middle two columns of the second row."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQD5E-dZMe0z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8c80a5b-7a87-4f41-ddd0-0ed1b7008c6d"
      },
      "source": [
        "b[1, 1:3]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([22, 23], dtype=torch.int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rVSaQyyMe0z"
      },
      "source": [
        "#### 15) Write a function named last_element that takes an (n,m) tensor (assume type torch.float32) as input (should support arbitrary n and m) and returns the last column of the last row, via indexing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHTSly57Me00"
      },
      "source": [
        "def last_element(tenSir : torch.tensor) -> torch.float32 :\r\n",
        "  return tenSir[-1, -1]\r\n",
        "\r\n",
        "#last_element(b)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1HSicMYMe00"
      },
      "source": [
        "#### 16) Write a function named remove_outliers that takes an (n,) tensor with type torch.float32 (lets call it x) as input (should support arbitrary n), computes the mean $\\mu$ and standard deviation $\\sigma$ over all elements in x, clips all elements smaller than $\\mu-\\sigma$ to be $\\mu-\\sigma$, and clips all elements larger than $\\mu+\\sigma$ to be $\\mu+\\sigma$, and returns the result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8Jr-uEAMe00"
      },
      "source": [
        "def remove_outliers(x : torch.tensor):\n",
        "    mu = torch.mean(x)\n",
        "    std = torch.std(x)\n",
        "\n",
        "    return torch.clip(x, mu-std, mu+std)\n",
        "\n",
        "\n",
        "#u = torch.empty(10000).normal_(mean = 1.5, std = 0.5) * 2\n",
        "\n",
        "#remove_outliers(u)\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIxP3sO3Me00"
      },
      "source": [
        "#### 17) Write a function named all_positive that takes an arbitrary tensor (assume type torch.float32) as input and returns true if and only if all elements in the tensor are greater than 0.0. Note: the original tensor passed it must not be modified after this function has run; test to makes sure this is true. Note: the original tensor passed it must not be modified after this function has run; test to makes sure this is true."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zggbHkDUMe01"
      },
      "source": [
        "def all_positive(x : torch.tensor):\n",
        "    return (torch.min(x) > 0.0)\n",
        "\n",
        "#all_positive(b)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAeJtFLEMe01"
      },
      "source": [
        "#### 18) Write a function named leaky_relu that takes an arbitrary tensor (assume type torch.float32) as input, and multiplies all and only the negative values in the tensor by 0.01, and then returns the result (do not use torch's built-in LeakyReLU functionality)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEYV40AkMe01"
      },
      "source": [
        "def leaky_relu(x : torch.tensor):\n",
        "    x[x < 0] = x[x < 0] * 0.01\n",
        "    return x\n",
        "\n",
        "#z = torch.empty(10).normal_(mean = 1.5, std = 0.5) * -2\n",
        "#print(z)\n",
        "#print(leaky_relu(z))"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bR8BwmSjMe01"
      },
      "source": [
        "#### 19) The softmax function takes a tensor $x \\in \\mathbb{R}^N$ as input and returns $y \\in \\mathbb{R}^N$ as output according to the following equation.\n",
        "$$ y_i = \\frac{e^{x_i}}{\\sum_{j=1}^N e^{x_j}} $$\n",
        "Write a function softmax that takes an (n,) tensor x (assume type tensor.float32) as input and returns y, the softmax of it. You must do this using vectorized operations: no loops!  Do not use torch's built-in softmax functionality."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jatqDyhdMe02"
      },
      "source": [
        "def softmax(x : torch.tensor):\n",
        "    return torch.exp(x) / torch.sum(torch.exp(x))\n",
        "\n",
        "\n",
        "#z = torch.empty(10).normal_(mean = 1.5, std = 0.5) * -2\n",
        "\n",
        "#sm = torch.nn.Softmax(dim=0)\n",
        "\n",
        "#print(z)\n",
        "#print(sm(z))\n",
        "#softmax(z)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRqbcHg5Me02"
      },
      "source": [
        "#### 20) Write a function what_why that takes as input an arbitrary shape tensor, finds the element containing the smallest value, then finds the element containing the largest value, and then swaps those two elements and returns the result.  Note: the original tensor passed it must not be modified after this function has run; test to makes sure this is true."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSEfA7ayMe02"
      },
      "source": [
        "def what_why(x : torch.tensor):\n",
        "    tensir = torch.clone(x)\n",
        "    minix, maxix = torch.argmin(tensir), torch.argmax(tensir)\n",
        "    minimum, maximum = torch.min(tensir), torch.max(tensir)\n",
        "    tensir[minix], tensir[maxix] = maximum, minimum\n",
        "    return tensir\n",
        "\n",
        "#print(a)\n",
        "#print(what_why(a))\n",
        "#print(a)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhmbDdTdMe03"
      },
      "source": [
        "#### 21) Sum b column-wise (yielding a row vector)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWnUI7jnMe03",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae9db899-404e-446e-da6f-d4f85975b3bf"
      },
      "source": [
        "torch.sum(b, axis=0)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([63, 66, 69, 72])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bWbZCEFMe03"
      },
      "source": [
        "#### 22) Compute the row-wise max of b."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmuYu9t1Me03",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a733efe-4f11-4206-9211-339d40d42693"
      },
      "source": [
        "torch.sum(b, axis=1)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 50,  90, 130])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J42NFitRMe03"
      },
      "source": [
        "#### 23) Create a function column_norm that takes an arbitrary (n,m) tensor (of type torch.float32) and column-wise normalizes it (so that each column sums to 1). Note: the original tensor passed it must not be modified after this function has run; test to makes sure this is true. Reminder: no loops!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFwSkpJYMe04"
      },
      "source": [
        "def column_norm(tenSir: torch.tensor) -> torch.tensor:\r\n",
        "  return tenSir / torch.sum(tenSir,axis=0)\r\n",
        "\r\n",
        "#print(b)\r\n",
        "#print(column_norm(b))\r\n",
        "#print(b)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KBIgTtwMe04"
      },
      "source": [
        "#### 24) Reshape a into a (1,10), hstack it with itself, and compute the dimensions of the resulting tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeZajO4YMe04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14731852-1d32-4d8c-9b5d-c6cc4773fd7c"
      },
      "source": [
        "torch.hstack((torch.reshape(a, (1,10)), torch.reshape(a, (1,10)))).shape"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 20])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qql0Wha1Me05"
      },
      "source": [
        "#### 25) hsplit b into two matrices of equal size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDFvuQbfMe05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee874087-4f99-4669-9188-7c0c41e2f81c"
      },
      "source": [
        "torch.split(b, 2, dim=1)\r\n",
        "\r\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[11, 12],\n",
              "         [21, 22],\n",
              "         [31, 32]], dtype=torch.int32), tensor([[13, 14],\n",
              "         [23, 24],\n",
              "         [33, 34]], dtype=torch.int32))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mm94Bw2VNdQz"
      },
      "source": [
        "### Part II: Tensor View and Broadcasting Semantics\n",
        "\n",
        "All students and all groups must complete the following problems.  Before you tackle them, please take a few minutes to read the following documents:\n",
        "\n",
        "https://pytorch.org/docs/stable/tensor_view.html\n",
        "\n",
        "https://pytorch.org/docs/stable/notes/broadcasting.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6xfxNNTXZ_j"
      },
      "source": [
        "#### 26) In the *text* box below, please briefly explain what a view of a tensor is and why views can be helpful."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqkw-8WaXaNk"
      },
      "source": [
        "A view is a copy of the tensor, but copied by reference. It is helpful for reshaping, slicing, and element-wise operations to tensors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GmuycB9X10q"
      },
      "source": [
        "#### 27) In the *text* box below, please briefly explain what it means for a tensor to be contiguous, and what the contiguous() function does."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRGrTyToX16N"
      },
      "source": [
        "For a tensor to be contiguous, it means the tensor will view or modify the tensor in place, where as non-contiguous would treat it like a copy. Contiguous() allows us to make a copy of a non-continguous tensor that is contiguous.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wH4r0DZUZ54z"
      },
      "source": [
        "#### 28) Give an example of using -1 as one of the dimensions in a call to reshape on b."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsR43wouZ6kh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af1f7699-ede0-48c0-ce03-ee6cc00fd649"
      },
      "source": [
        "torch.reshape(b, (-1,))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([11, 12, 13, 14, 21, 22, 23, 24, 31, 32, 33, 34], dtype=torch.int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbOQakh4Wv7Z"
      },
      "source": [
        "#### 29) Check whether b is contiguous, then whether the transpose of b is contiguous, and then whether the transpose of the transpose of b is contiguous. Print each check, so your output should be three booleans in a row."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jctu3gPOWwCZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51c19667-5ffc-4a6e-95e5-4312a5e2b0fa"
      },
      "source": [
        "print(b.is_contiguous())\r\n",
        "print(torch.transpose(b, 0, 1).is_contiguous())\r\n",
        "print(torch.transpose(torch.transpose(b, 0, 1), 0, 1).is_contiguous())"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "False\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tb7nFRNtRm-o"
      },
      "source": [
        "#### 30) Use the permute function on b with the right arguments to produce a tensor that is equal to the transpose of b.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_DiZQ4DRjqK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e87eba10-0fbd-4b86-af05-ed58b4b52792"
      },
      "source": [
        "print(torch.transpose(b, 0,1))\r\n",
        "print(b.permute(1,0))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[11, 21, 31],\n",
            "        [12, 22, 32],\n",
            "        [13, 23, 33],\n",
            "        [14, 24, 34]], dtype=torch.int32)\n",
            "tensor([[11, 21, 31],\n",
            "        [12, 22, 32],\n",
            "        [13, 23, 33],\n",
            "        [14, 24, 34]], dtype=torch.int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NoA2bOKUHA5"
      },
      "source": [
        "#### 31) Generate a random (any flavor of random will do) tensor of $C \\times N \\times H \\times W = 3 \\times 100 \\times 32 \\times 32$. We will pretend this is a set of $N=100$ images, each $H=32$ pixels high and $W=32$ pixels wide, and each with $C=3$ color channels (red, green, blue). If we needed to feed this into a function that assumed the shape of the tensor was $(N,C,H,W)$, should you use view, reshape, tranpose or permute? In the text box below, please briefly explain which you should use and why. Then, in the code box below it, demonstrate its use to reorgnize the data so it is in the arrangment the function expects."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niXraCMzZKCN"
      },
      "source": [
        "We decided to use view because we wanted to avoid uneccesary data copies, which helps us mitigate inefficient memory usage. We know that we're inputting a contiguous tensor, but it's unknown what the function itself will do, so if the function doesn't perform an operation that would make it non-contiguous, then the tensor will stay contiguous throughout the function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9uFYbolUGrv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57eae1af-f2bf-4a6a-b057-3354b3fd9c23"
      },
      "source": [
        "x = torch.empty((3, 100, 32, 32)).normal_(mean = 1.5, std = 0.5)\r\n",
        "print(x.shape)\r\n",
        "print(x.view((100, 3, 32, 32)).shape)\r\n",
        "print(x.shape)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 100, 32, 32])\n",
            "torch.Size([100, 3, 32, 32])\n",
            "torch.Size([3, 100, 32, 32])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pf5AIK7fYbpd"
      },
      "source": [
        "#### 32) Generate the same random $C \\times N \\times H \\times W = 3 \\times 100 \\times 32 \\times 32$ tensor as in the previous problem. This time we want to feed this into a model that assumes each image is stacked into one long vector (i.e. each image is stacked into a long [length 3072] vector). Demonstrate the operation, or series of operations, needed to create the $(100,3072)$ matrix of flattened images. Note that your operations are incorrect if information from one image leaks into another: the $i$th row of the result should contain all and only the RGB pixel intensities for the $i$th image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rWtE9xkYa15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71768cd3-6f2c-4f40-84c8-418a67d09176"
      },
      "source": [
        "x = torch.empty((3, 100, 32, 32)).normal_(mean = 1.5, std = 0.5)\r\n",
        "print(x.shape)\r\n",
        "print(x.view((100, 3072)).shape)\r\n",
        "print(x.shape)\r\n",
        "print(x.view((100, 3, 32, 32))[0].sum())\r\n",
        "print(x.view((100, 3072))[0].sum())"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 100, 32, 32])\n",
            "torch.Size([100, 3072])\n",
            "torch.Size([3, 100, 32, 32])\n",
            "tensor(4623.5283)\n",
            "tensor(4623.5283)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiRFk6fodMkc"
      },
      "source": [
        "#### 33) In the text box below it, explain what broadcasting is taking place  happenening in the following code box.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j09TttOHdM3B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95e62d86-0d6e-4e5e-a346-27f451efdffe"
      },
      "source": [
        "b + torch.tensor([1,2,3,4])\r\n",
        "print(b)\r\n",
        "print(b + torch.tensor([1,2,3,4]))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[11, 12, 13, 14],\n",
            "        [21, 22, 23, 24],\n",
            "        [31, 32, 33, 34]], dtype=torch.int32)\n",
            "tensor([[12, 14, 16, 18],\n",
            "        [22, 24, 26, 28],\n",
            "        [32, 34, 36, 38]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1O_x7hDeBTg"
      },
      "source": [
        "The broadcasting allows us to apply the operations of uneven sizes to all elements in a given tensor, all rows, or all columns. In this case the broadcasting applys the row addition to all rows of b. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlOAhZ7feEnz"
      },
      "source": [
        "#### 34) In the text box below it, explain what broadcasting is taking place in the following code box? Also, why did I put transposes there? Why couldn't I just to b + torch.tensor([1,2,3])?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dEKYpbzd90X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d494154-166b-43d1-d858-41a18827e5ad"
      },
      "source": [
        "(b.T + torch.tensor([1,2,3])).T"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[12, 13, 14, 15],\n",
              "        [23, 24, 25, 26],\n",
              "        [34, 35, 36, 37]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDDCnv66erQ9"
      },
      "source": [
        "The broadcasting here is very similar to above, but with the transpose operations we are broadcasting across the columns instead of the rows. If you did \"b + torch.tensor([1,2,3])\" then you would get a dimension mismatch as it would try to broadcast across the rows of size 4, rather than the columns of size 3. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_LU_vhwe5Na"
      },
      "source": [
        "#### 35) In the text box below it, explain each of the following three torch.equal results. Why are the equal things equal, and the unequal things unequal?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1zzb7q0eMRq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e8c9485-4b04-42db-8bfb-901ba1422d22"
      },
      "source": [
        "z = torch.randn(10,1)\n",
        "print(torch.equal(z @ z.T,z * z.T))\n",
        "print(torch.equal(z.T @ z,z.T * z))\n",
        "print(torch.equal(z.T * z,z * z.T))\n",
        "#print(z)\n",
        "#print(z.T)\n",
        "#print(z.T @ z)\n",
        "#print(z.T * z)\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "False\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCrQLSokgUx4"
      },
      "source": [
        "When using matrix multiplication the dimensions of the two input matrices determines the size of the output matrix. In the case of the \"\\*\" operation python is attempting to broadcast the multiplication operation between the two matrices.  In the case where `z.T @ z != z.T * z` we see this effect as the output shape of `z.T @ z` is a 1x1 while `z.T * z` is a 10x10. In the case where some of these are equal to each other it holds the same properties as described before, just in the case that the dimensions of the result end up being the same."
      ]
    }
  ]
}